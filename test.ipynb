{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fattahConfig {\n",
       "  \"_attn_implementation_autoset\": true,\n",
       "  \"activation_function\": \"silu\",\n",
       "  \"attention_bias\": false,\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"aux_loss_coef\": 0.01,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"hidden_size\": 576,\n",
       "  \"initializer_range\": 0.01,\n",
       "  \"input_bits\": 8,\n",
       "  \"intermediate_size\": 1536,\n",
       "  \"kv_channels\": 128,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"fattah\",\n",
       "  \"num_attention_heads\": 10,\n",
       "  \"num_experts_per_tok\": 2,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"num_key_value_heads\": 5,\n",
       "  \"num_local_experts\": 6,\n",
       "  \"output_router_logits\": false,\n",
       "  \"pretraining_tp\": 1,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_scaling\": null,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.46.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 41447,\n",
       "  \"weight_bits\": 1\n",
       "}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from src.architecture import configuration_fattah, modeling_fattah\n",
    "# Reload the modules\n",
    "importlib.reload(configuration_fattah)\n",
    "importlib.reload(modeling_fattah)\n",
    "from src.architecture.configuration_fattah import fattahConfig\n",
    "from src.architecture.modeling_fattah import fattahModel\n",
    "configuration = fattahConfig()\n",
    "model = fattahModel(configuration)\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_params:  150.446592 \n",
      "Trainable parameters: 150.446592\n"
     ]
    }
   ],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"total_params: \",total_params/1000000,\"\\nTrainable parameters:\", trainable_params/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.92928"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(p.numel() for p in model.layers[0].mlp.parameters())/1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.362176, 2.3616)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(sum(p.numel() for p in model.layers[0].self_attention.experts.parameters())/1000000  ) ,(sum(p.numel() for p in model.layers[0].self_attention.experts.input_linear.parameters())/1000000 +sum(p.numel() for p in model.layers[0].self_attention.experts.output_linear.parameters())/1000000  +sum(p.numel() for p in model.layers[0].self_attention.experts.router.parameters())/1000000 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5898240000000001, 2.362176)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = sum(p.numel() for p in model.embed_tokens.parameters()) # vocab * hidden_size\n",
    "\n",
    "atten_kvproj = sum(p.numel() for p in model.layers[0].self_attention.parameters()) /1000000 - sum(p.numel() for p in model.layers[0].self_attention.experts.parameters())/1000000\n",
    "atten_kvproj,sum(p.numel() for p in model.layers[0].self_attention.experts.parameters())/1000000 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['embed_tokens.weight', 'layers.0.input_layernorm.weight', 'layers.0.self_attention.experts.bias', 'layers.0.self_attention.experts.input_linear.weight', 'layers.0.self_attention.experts.output_linear.weight', 'layers.0.self_attention.experts.router.layer.weight', 'layers.0.self_attention.kv_proj.weight', 'layers.0.post_attention_layernorm.weight', 'layers.0.mlp.bias', 'layers.0.mlp.input_linear.weight', 'layers.0.mlp.output_linear.weight', 'layers.0.mlp.router.layer.weight', 'layers.1.input_layernorm.weight', 'layers.1.self_attention.experts.bias', 'layers.1.self_attention.experts.input_linear.weight', 'layers.1.self_attention.experts.output_linear.weight', 'layers.1.self_attention.experts.router.layer.weight', 'layers.1.self_attention.kv_proj.weight', 'layers.1.post_attention_layernorm.weight', 'layers.1.mlp.bias', 'layers.1.mlp.input_linear.weight', 'layers.1.mlp.output_linear.weight', 'layers.1.mlp.router.layer.weight', 'layers.2.input_layernorm.weight', 'layers.2.self_attention.experts.bias', 'layers.2.self_attention.experts.input_linear.weight', 'layers.2.self_attention.experts.output_linear.weight', 'layers.2.self_attention.experts.router.layer.weight', 'layers.2.self_attention.kv_proj.weight', 'layers.2.post_attention_layernorm.weight', 'layers.2.mlp.bias', 'layers.2.mlp.input_linear.weight', 'layers.2.mlp.output_linear.weight', 'layers.2.mlp.router.layer.weight', 'layers.3.input_layernorm.weight', 'layers.3.self_attention.experts.bias', 'layers.3.self_attention.experts.input_linear.weight', 'layers.3.self_attention.experts.output_linear.weight', 'layers.3.self_attention.experts.router.layer.weight', 'layers.3.self_attention.kv_proj.weight', 'layers.3.post_attention_layernorm.weight', 'layers.3.mlp.bias', 'layers.3.mlp.input_linear.weight', 'layers.3.mlp.output_linear.weight', 'layers.3.mlp.router.layer.weight', 'layers.4.input_layernorm.weight', 'layers.4.self_attention.experts.bias', 'layers.4.self_attention.experts.input_linear.weight', 'layers.4.self_attention.experts.output_linear.weight', 'layers.4.self_attention.experts.router.layer.weight', 'layers.4.self_attention.kv_proj.weight', 'layers.4.post_attention_layernorm.weight', 'layers.4.mlp.bias', 'layers.4.mlp.input_linear.weight', 'layers.4.mlp.output_linear.weight', 'layers.4.mlp.router.layer.weight', 'layers.5.input_layernorm.weight', 'layers.5.self_attention.experts.bias', 'layers.5.self_attention.experts.input_linear.weight', 'layers.5.self_attention.experts.output_linear.weight', 'layers.5.self_attention.experts.router.layer.weight', 'layers.5.self_attention.kv_proj.weight', 'layers.5.post_attention_layernorm.weight', 'layers.5.mlp.bias', 'layers.5.mlp.input_linear.weight', 'layers.5.mlp.output_linear.weight', 'layers.5.mlp.router.layer.weight', 'norm.weight'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('model',safe_serialization=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: what is your name?\n",
      "Tokenized Sentence: {'input_ids': [1, 825, 338, 596, 1024, 29973, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaTokenizer\n",
    "tokenizer = LlamaTokenizer.from_pretrained(r\"src\\tokenizer\\llama_tokenizer\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "sample_sentence = \"what is your name?\"\n",
    "tokens = tokenizer(\n",
    "                sample_sentence, truncation=True,\n",
    "                padding='max_length', max_length=16)\n",
    "print(f\"Original Sentence: {sample_sentence}\\nTokenized Sentence: {tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    Trainer, TrainingArguments, DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import load_dataset\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def create_tokenized_dataset_splits(path, tokenizer, block_size):\n",
    "    dataset = load_dataset('text', data_files=path)\n",
    "    shuffled_dataset = dataset['train'].shuffle(seed=42)\n",
    "    split_datasets = shuffled_dataset.train_test_split(test_size=0.2)\n",
    "\n",
    "    def tokenize_dataset(dataset):\n",
    "        return dataset.map(\n",
    "            lambda examples: tokenizer(\n",
    "                examples['text'], truncation=True,\n",
    "                padding='max_length', max_length=block_size\n",
    "            ),\n",
    "            batched=True\n",
    "        )\n",
    "\n",
    "    def unique_name_set(dataset):\n",
    "      names_set = set()\n",
    "\n",
    "      for example in dataset:\n",
    "          name = example['text'].split(\".\")[0]\n",
    "          names_set.add(name)\n",
    "\n",
    "      return names_set\n",
    "\n",
    "    return tokenize_dataset(split_datasets['train']), tokenize_dataset(split_datasets['test']), unique_name_set(split_datasets['train'])\n",
    "\n",
    "def train_model(model, tokenizer, train_dataset, test_dataset, out_folder_path):\n",
    "    data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=out_folder_path,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs=100,\n",
    "        per_device_train_batch_size=8,\n",
    "        save_steps=10000,\n",
    "        logging_steps=10,\n",
    "        eval_steps=1000,\n",
    "        logging_dir=f'{out_folder_path}/logs',\n",
    "        evaluation_strategy=\"steps\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.001)]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    model.save_pretrained(out_folder_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = create_config_model(out_folder_path)\n",
    "train_dataset, test_dataset, unique_names = create_tokenized_dataset_splits('/content/drive/MyDrive/LLM_Projects/llama2_project/dataset/pt_data_txt/custom/pt_alpaca_in_text.txt', tokenizer, block_size=32)\n",
    "train_model(model, tokenizer, train_dataset, test_dataset, out_folder_path)\n",
    "\n",
    "print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "It looks like I started in the middle of a conversation. Let's start fresh!**Welcome to our conversation!**I'm Athena2, your friendly AI assistant. I'd love to chat with you about any topic that interests you - from blockchain and AI to machine learning and data science. Feel free to share what's on your mind, and I'll do my best to break down complex ideas into fun, relatable conversations.**To get us started, would you like to:**1. **Explore a specific tech topic** (e.g., \"What is the impact of AI in healthcare?\").2. **Discuss a broader theme** (e.g., \"The future of decentralized systems\").3. **Just geek out about tech trends** (no specific question needed - we can just enjoy the conversation!).4. **Something else** (please share, and I'll do my best to accommodate).Please respond with one of the numbers above (1,2,3, or \"Something else\"), or feel free to ask a question right away!\n",
    "\"It looks like I started in the middle of a conversation. Let's start fresh!**Welcome to our conversation!**I'm Athena2, your friendly AI assistant. I'd love to chat with you about any topic that interests you - from blockchain and AI to machine learning and data science. Feel free to share what's on your mind, and I'll do my best to break down complex ideas into fun, relatable conversations.**To get us started, would you like to:**1. **Explore a specific tech topic** (e.g., \\\"What is the impact of AI in healthcare?\\\").2. **Discuss a broader theme** (e.g., \\\"The future of decentralized systems\\\").3. **Just geek out about tech trends** (no specific question needed - we can just enjoy the conversation!).4. **Something else** (please share, and I'll do my best to accommodate).Please respond with one of the numbers above (1,2,3, or \\\"Something else\\\"), or feel free to ask a question right away!\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
